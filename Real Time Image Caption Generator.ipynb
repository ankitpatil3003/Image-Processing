{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ndK6Z50d_qhQ","outputId":"75382f9a-12aa-4a55-e3d0-8925bccf46ce","colab":{"referenced_widgets":["5b3aa3a42ab1441cb64ff1c8f3adc8d1"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_8744\\2998871579.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  tqdm().pandas()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b3aa3a42ab1441cb64ff1c8f3adc8d1","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import string\n","import numpy as np\n","from PIL import Image\n","import os\n","from pickle import dump, load\n","import numpy as np\n","\n","from keras.applications.xception import Xception, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.layers.merge import add\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n","\n","from tqdm import tqdm_notebook as tqdm\n","tqdm().pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHAxNj_Z_qhg"},"outputs":[],"source":["def load_doc(filename):\n","    file = open(filename, 'r')\n","    text = file.read()\n","    file.close()\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7W9So3a_qhi"},"outputs":[],"source":["def all_img_captions(filename):\n","    file = load_doc(filename)\n","    captions = file.split('\\n')\n","    descriptions ={}\n","    for caption in captions[:-1]:\n","        img, caption = caption.split('\\t')\n","        if img[:-2] not in descriptions:\n","            descriptions[img[:-2]] = [ caption ]\n","        else:\n","            descriptions[img[:-2]].append(caption)\n","    return descriptions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6fDRS02_qhl"},"outputs":[],"source":["def cleaning_text(captions):\n","    table = str.maketrans('','',string.punctuation)\n","    for img,caps in captions.items():\n","        for i,img_caption in enumerate(caps):\n","\n","            img_caption.replace(\"-\",\" \")\n","            desc = img_caption.split()\n","\n","            desc = [word.lower() for word in desc]\n","            desc = [word.translate(table) for word in desc]\n","            desc = [word for word in desc if(len(word)>1)]\n","            desc = [word for word in desc if(word.isalpha())]\n","\n","            img_caption = ' '.join(desc)\n","            captions[img][i]= img_caption\n","    return captions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"807KAe17_qhm"},"outputs":[],"source":["def text_vocabulary(descriptions):\n","    vocab = set()\n","\n","    for key in descriptions.keys():\n","        [vocab.update(d.split()) for d in descriptions[key]]\n","\n","    return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgLnMPSQ_qho"},"outputs":[],"source":["def save_descriptions(descriptions, filename):\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + '\\t' + desc )\n","    data = \"\\n\".join(lines)\n","    file = open(filename,\"w\")\n","    file.write(data)\n","    file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0Vu1y1__qhp"},"outputs":[],"source":["dataset_text = \"D:\\Project RTICG\\Flickr_8k_text\"\n","dataset_images = \"D:\\Project RTICG\\Flickr8k_Dataset\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExhmgPQX_qhr","outputId":"598d508d-af7a-4a5b-d2e0-79e6770edee4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of descriptions = 8092\n"]}],"source":["filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n","descriptions = all_img_captions(filename)\n","print(\"Length of descriptions =\" ,len(descriptions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIDWOJ6j_qht"},"outputs":[],"source":["clean_descriptions = cleaning_text(descriptions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9UAz_As_qhu","outputId":"f3da8aa5-da1b-48d6-c3ae-05abaee1cc4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of vocabulary =  8763\n"]}],"source":["vocabulary = text_vocabulary(clean_descriptions)\n","print(\"Length of vocabulary = \", len(vocabulary))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IP8LEDat_qhw"},"outputs":[],"source":["save_descriptions(clean_descriptions, \"descriptions.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPZ_sWep_qhw"},"outputs":[],"source":["def extract_features(directory):\n","        model = Xception( include_top=False, pooling='avg' )\n","        features = dict()\n","        for img in tqdm(os.listdir(directory)):\n","            filename = directory + \"/\" + img\n","            image = Image.open(filename)\n","            image = image.resize((299,299))\n","            image = np.expand_dims(image, axis=0)\n","            image = image/127.5\n","            image = image - 1.0\n","\n","            feature = model.predict(image)\n","            features[img] = feature\n","        return features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RELQxnOr_qhx","outputId":"38bff793-587c-4590-b410-8cc9467eb3b8","colab":{"referenced_widgets":["84df4ea0798e43d4b2a779f6ea0fe1ff"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\ankit\\AppData\\Local\\Temp/ipykernel_15000/2766766201.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for img in tqdm(os.listdir(directory)):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84df4ea0798e43d4b2a779f6ea0fe1ff","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8091 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["features = extract_features(dataset_images)\n","dump(features, open(\"features.p\",\"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRUbUwra_qhy"},"outputs":[],"source":["features = load(open(\"features.p\",\"rb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ogjr-B-_qhz"},"outputs":[],"source":["def load_photos(filename):\n","    file = load_doc(filename)\n","    photos = file.split(\"\\n\")[:-1]\n","    return photos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0j29AYsa_qhz"},"outputs":[],"source":["def load_clean_descriptions(filename, photos):\n","    file = load_doc(filename)\n","    descriptions = {}\n","    for line in file.split(\"\\n\"):\n","\n","        words = line.split()\n","        if len(words)<1 :\n","            continue\n","\n","        image, image_caption = words[0], words[1:]\n","\n","        if image in photos:\n","            if image not in descriptions:\n","                descriptions[image] = []\n","            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n","            descriptions[image].append(desc)\n","\n","    return descriptions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPRN1Bnz_qh0"},"outputs":[],"source":["def load_features(photos):\n","    all_features = load(open(\"features.p\",\"rb\"))\n","    features = {k:all_features[str(k)] for k in photos}\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EOq6VHo_qh0"},"outputs":[],"source":["filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n","\n","train_imgs = load_photos(filename)\n","train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n","train_features = load_features(train_imgs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qbc87wM__qh1"},"outputs":[],"source":["def dict_to_list(descriptions):\n","    all_desc = []\n","    for key in descriptions.keys():\n","        [all_desc.append(d) for d in descriptions[key]]\n","    return all_desc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XqM3HeW_qh1"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","\n","def create_tokenizer(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(desc_list)\n","    return tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yG8ITL__qh2","outputId":"e1596797-2c20-4bd0-eca8-bde7852e0ba9"},"outputs":[{"data":{"text/plain":["7577"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = create_tokenizer(train_descriptions)\n","dump(tokenizer, open('tokenizer.p', 'wb'))\n","vocab_size = len(tokenizer.word_index) + 1\n","vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjnZ3RSq_qh3","outputId":"c3033037-509c-407e-d421-8189d1df3d08"},"outputs":[{"data":{"text/plain":["38"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["def max_length(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    return max(len(d.split()) for d in desc_list)\n","\n","max_length = max_length(descriptions)\n","max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zGT21FD_qh3"},"outputs":[],"source":["def data_generator(descriptions, features, tokenizer, max_length):\n","    while 1:\n","        for key, description_list in descriptions.items():\n","            feature = features[key][0]\n","            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n","            yield [[input_image, input_sequence], output_word]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2bZiIX7_qh4"},"outputs":[],"source":["def create_sequences(tokenizer, max_length, desc_list, feature):\n","    X1, X2, y = list(), list(), list()\n","    for desc in desc_list:\n","        seq = tokenizer.texts_to_sequences([desc])[0]\n","        for i in range(1, len(seq)):\n","            in_seq, out_seq = seq[:i], seq[i]\n","            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","            X1.append(feature)\n","            X2.append(in_seq)\n","            y.append(out_seq)\n","    return np.array(X1), np.array(X2), np.array(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7U7bPnXf_qh5","outputId":"38b0f87c-65b1-4777-e07f-7b66320b2316"},"outputs":[{"data":{"text/plain":["((47, 2048), (47, 38), (47, 7577))"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n","a.shape, b.shape, c.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGExWvxn_qh5"},"outputs":[],"source":["from tensorflow.keras.utils import plot_model\n","\n","def define_model(vocab_size, max_length):\n","\n","    inputs1 = Input(shape=(2048,))\n","    fe1 = Dropout(0.5)(inputs1)\n","    fe2 = Dense(256, activation='relu')(fe1)\n","\n","    inputs2 = Input(shape=(max_length,))\n","    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","    se2 = Dropout(0.5)(se1)\n","    se3 = LSTM(256)(se2)\n","\n","    decoder1 = add([fe2, se3])\n","    decoder2 = Dense(256, activation='relu')(decoder1)\n","    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","    print(model.summary())\n","    plot_model(model, to_file='model.png', show_shapes=True)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GouBgxmC_qh6","outputId":"9c20eb25-18f0-4161-e7a5-82fa6dc3898a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset:  6000\n","Descriptions: train= 6000\n","Photos: train= 6000\n","Vocabulary Size: 7577\n","Description Length:  38\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 38)]         0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 2048)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 38, 256)      1939712     ['input_3[0][0]']                \n","                                                                                                  \n"," dropout (Dropout)              (None, 2048)         0           ['input_2[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 38, 256)      0           ['embedding[0][0]']              \n","                                                                                                  \n"," dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n","                                                                                                  \n"," add_12 (Add)                   (None, 256)          0           ['dense[0][0]',                  \n","                                                                  'lstm[0][0]']                   \n","                                                                                                  \n"," dense_1 (Dense)                (None, 256)          65792       ['add_12[0][0]']                 \n","                                                                                                  \n"," dense_2 (Dense)                (None, 7577)         1947289     ['dense_1[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,002,649\n","Trainable params: 5,002,649\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\ankit\\AppData\\Local\\Temp/ipykernel_15000/4078830559.py:13: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n"]},{"name":"stdout","output_type":"stream","text":["6000/6000 [==============================] - 968s 161ms/step - loss: 4.4982\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\ankit\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n"]},{"name":"stdout","output_type":"stream","text":["6000/6000 [==============================] - 994s 166ms/step - loss: 3.6541\n","6000/6000 [==============================] - 989s 165ms/step - loss: 3.3666\n","6000/6000 [==============================] - 984s 164ms/step - loss: 3.1953\n","6000/6000 [==============================] - 989s 165ms/step - loss: 3.0775\n","6000/6000 [==============================] - 983s 164ms/step - loss: 2.9872\n","6000/6000 [==============================] - 982s 164ms/step - loss: 2.9216\n","6000/6000 [==============================] - 981s 164ms/step - loss: 2.8656\n","6000/6000 [==============================] - 989s 165ms/step - loss: 2.8238\n","6000/6000 [==============================] - 992s 165ms/step - loss: 2.7845\n","6000/6000 [==============================] - 991s 165ms/step - loss: 2.7569\n","6000/6000 [==============================] - 1001s 167ms/step - loss: 2.7285\n","6000/6000 [==============================] - 995s 166ms/step - loss: 2.7106\n","6000/6000 [==============================] - 988s 165ms/step - loss: 2.6916\n","6000/6000 [==============================] - 1007s 168ms/step - loss: 2.6843\n","6000/6000 [==============================] - 986s 164ms/step - loss: 2.6688\n","6000/6000 [==============================] - 988s 165ms/step - loss: 2.6583\n","6000/6000 [==============================] - 988s 165ms/step - loss: 2.6459\n","6000/6000 [==============================] - 995s 166ms/step - loss: 2.6360\n","6000/6000 [==============================] - 997s 166ms/step - loss: 2.6329\n","6000/6000 [==============================] - 997s 166ms/step - loss: 2.6284\n","6000/6000 [==============================] - 975s 162ms/step - loss: 2.6212\n","6000/6000 [==============================] - 974s 162ms/step - loss: 2.6182\n","6000/6000 [==============================] - 976s 163ms/step - loss: 2.6153\n","6000/6000 [==============================] - 977s 163ms/step - loss: 2.6098\n","6000/6000 [==============================] - 979s 163ms/step - loss: 2.6082\n","6000/6000 [==============================] - 974s 162ms/step - loss: 2.6078\n","6000/6000 [==============================] - 971s 162ms/step - loss: 2.6038\n","6000/6000 [==============================] - 973s 162ms/step - loss: 2.6048\n","6000/6000 [==============================] - 972s 162ms/step - loss: 2.6007s\n","6000/6000 [==============================] - 966s 161ms/step - loss: 2.6017\n","6000/6000 [==============================] - 967s 161ms/step - loss: 2.6015\n","6000/6000 [==============================] - 959s 160ms/step - loss: 2.5991\n","6000/6000 [==============================] - 958s 160ms/step - loss: 2.6034\n","6000/6000 [==============================] - 959s 160ms/step - loss: 2.6006\n","6000/6000 [==============================] - 959s 160ms/step - loss: 2.5999\n","6000/6000 [==============================] - 956s 159ms/step - loss: 2.5992\n","6000/6000 [==============================] - 959s 160ms/step - loss: 2.6027\n","6000/6000 [==============================] - 960s 160ms/step - loss: 2.6041\n","6000/6000 [==============================] - 959s 160ms/step - loss: 2.6026\n","6000/6000 [==============================] - 970s 162ms/step - loss: 2.6064\n","6000/6000 [==============================] - 966s 161ms/step - loss: 2.6070\n","6000/6000 [==============================] - 959s 160ms/step - loss: 2.6012\n","6000/6000 [==============================] - 960s 160ms/step - loss: 2.6096\n","6000/6000 [==============================] - 958s 160ms/step - loss: 2.6096\n","6000/6000 [==============================] - 958s 160ms/step - loss: 2.6124\n","6000/6000 [==============================] - 957s 160ms/step - loss: 2.6145\n","6000/6000 [==============================] - 959s 160ms/step - loss: 2.6144\n","6000/6000 [==============================] - 956s 159ms/step - loss: 2.6204\n","6000/6000 [==============================] - 960s 160ms/step - loss: 2.6173\n","6000/6000 [==============================] - 965s 161ms/step - loss: 2.6229\n","6000/6000 [==============================] - 956s 159ms/step - loss: 2.6201\n","6000/6000 [==============================] - 954s 159ms/step - loss: 2.6272\n","6000/6000 [==============================] - 955s 159ms/step - loss: 2.6292\n","6000/6000 [==============================] - 952s 159ms/step - loss: 2.6357\n","6000/6000 [==============================] - 951s 158ms/step - loss: 2.6333\n","6000/6000 [==============================] - 956s 159ms/step - loss: 2.6378\n","6000/6000 [==============================] - 954s 159ms/step - loss: 2.6468\n","6000/6000 [==============================] - 949s 158ms/step - loss: 2.6417\n","6000/6000 [==============================] - 950s 158ms/step - loss: 2.6423\n"]}],"source":["print('Dataset: ', len(train_imgs))\n","print('Descriptions: train=', len(train_descriptions))\n","print('Photos: train=', len(train_features))\n","print('Vocabulary Size:', vocab_size)\n","print('Description Length: ', max_length)\n","\n","model = define_model(vocab_size, max_length)\n","epochs = 60\n","steps = len(train_descriptions)\n","os.mkdir(\"models\")\n","for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n","    model.save(\"models/model_\" + str(i) + \".h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GH5HlWYw_qh7"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}